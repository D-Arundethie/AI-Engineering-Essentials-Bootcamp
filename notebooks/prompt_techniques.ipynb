{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7e2d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..') # One step back from the currect direction\n",
    "\n",
    "from utils.prompts import render # Visualized the prompts\n",
    "from utils.llm_client import LLMClient # pick LLM Client\n",
    "from utils.logging_utils import log_llm_call # logging every API call\n",
    "from utils.router import pick_model, should_use_reasoning_model # General, reasoning or strong\n",
    "from IPython.display import Image, display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38cbb33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\Downloads\\\\ZUU CREW\\\\AI Engineer Essentials\\\\Codes\\\\Week 01\\\\notebooks'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce7ca8",
   "metadata": {},
   "source": [
    "### 01 - Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7581127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: gpt-4o-mini\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'The sentiment is: positive',\n",
       " 'usage': {'input_tokens_est': 47,\n",
       "  'context_tokens_est': 0,\n",
       "  'total_est': 50,\n",
       "  'prompt_tokens_actual': 50,\n",
       "  'completion_tokens_actual': 5,\n",
       "  'total_tokens_actual': 55},\n",
       " 'latency_ms': 2160,\n",
       " 'raw': ChatCompletion(id='chatcmpl-CsXIPkVjYNLxbMwqJNB2mDWO3LFAb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The sentiment is: positive', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1767113737, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_3683ee3deb', usage=CompletionUsage(completion_tokens=5, prompt_tokens=50, total_tokens=55, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))),\n",
       " 'meta': {'retry_count': 0, 'backoff_ms_total': 0, 'overflow_handled': False}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text, spec = render(\n",
    "                        \"zero_shot.v1\",\n",
    "                        role = \"sentiment_analysis\",\n",
    "                        Instruction = \"Analyze the following text determine the sentiment as positive / negetive / neutral.\",\n",
    "                        constrains = \"The sentiment should be one of the following: positive, negetive, neutral.\",\n",
    "                        format = \"The sentiment is: {sentiment}\"\n",
    "                        )\n",
    "\n",
    "model = pick_model(\"openai\", \"general\") # Hard to pick general or reasoning but choose general because detecting sentiment is not that logical, it's mainly depends on the keywords.\n",
    "print(\"Selected model:\", model)\n",
    "llm = LLMClient(\"openai\", model)\n",
    "\n",
    "text = \"I'm really happy with the product! It's amazing!\"\n",
    "messages = [\n",
    "            {\n",
    "                \"role\" : \"user\",\n",
    "                \"content\" : f\"{prompt_text}\\n\\nReview: {text}\"\n",
    "            }\n",
    "]\n",
    "\n",
    "llm.chat(messages, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32749e32",
   "metadata": {},
   "source": [
    "### 02 - Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "908b5b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is: neutral  \n",
      "Explanation: User expresses that the product seems nice but indicates it is not suitable for them, leading to a neutral sentiment.\n"
     ]
    }
   ],
   "source": [
    "examples = \"\"\"\n",
    "Example 01:\n",
    "Review: I'm really happy with the product! It's amazing!\n",
    "Sentiment: positive\n",
    "Explanation: User says product is good also he is happy.\n",
    "\n",
    "Example 02:\n",
    "Review: I'm really unhappy with the product! It's bad!\n",
    "Sentiment: negetive\n",
    "Explanation: User says product is bad and he is unhappy.\n",
    "\n",
    "Example 03:\n",
    "Review: The product is okay, but It's not great.\n",
    "Sentiment: neutral\n",
    "Explanation: User says product is okay but not great.\n",
    "\n",
    "Example 04:\n",
    "Review: I'm not sure about the product.\n",
    "Sentiment: neutral\n",
    "Explanation: User is not sure about the product.\n",
    "\n",
    "Example 05:\n",
    "Review: I'm really happy with the product! It's bad!\n",
    "Sentiment: negetive\n",
    "Explanation: User says product is good but he is unhappy. So we are prioratizing his words on product quality, not his happiness. That's why we are considering the product quality as negative.\n",
    "\n",
    "Example 06:\n",
    "Review: I'm really unhappy with the product but It's amazing!\n",
    "Sentiment: positive\n",
    "Explanation: User says product is bad but he is happy. So we are prioratizing his words on product quality, not his happiness. That's why we are considering the product quality as positive.\n",
    "\"\"\"\n",
    "\n",
    "prompt_text, spec = render(\n",
    "                        \"few_shot.v1\",\n",
    "                        role = \"sentiment_analysis\", \n",
    "                        examples = examples,\n",
    "                        Instruction = \"Analyze the following text determine the sentiment as positive / negetive / neutral. Also provide the explanation for the sentiment.\",\n",
    "                        constrains = \"The sentiment should be one of the following: positive, negetive, neutral.\",\n",
    "                        format = \"The sentiment is: {sentiment}\"\n",
    "                        )\n",
    "\n",
    "model = pick_model(\"openai\", \"general\")\n",
    "llm = LLMClient(\"openai\", model)\n",
    "\n",
    "text = \"It seems nice but It's not for me.\"\n",
    "messages = [\n",
    "            {\n",
    "                \"role\" : \"user\",\n",
    "                \"content\" : f\"{prompt_text}\\n\\nReview: {text}\"\n",
    "            }\n",
    "]\n",
    "\n",
    "response = llm.chat(messages, temperature=0.0)\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb92b98",
   "metadata": {},
   "source": [
    "### 03 - COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning Steps:\n",
      "1. For the non-stop case, use the formula: speed = total distance / total time = 100 miles / 2 hours.\n",
      "2. For the case with a 40-minute stop, subtract the stopping time from the total time to get the actual driving time. Convert 40 minutes to hours (40/60 = 2/3 hours) so the effective travel time becomes 2 – 2/3 = 4/3 hours.\n",
      "3. Then, use the formula: speed = total distance / (total travel time – stopping time).\n",
      "\n",
      "Answer:\n",
      "• Without stopping: Average speed = 100 ÷ 2 = 50 mph.\n",
      "• With a 40-minute stop: Average speed = 100 ÷ (4/3) = 75 mph.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Reasoning Steps:\n",
       "1. For the non-stop case, use the formula: speed = total distance / total time = 100 miles / 2 hours.\n",
       "2. For the case with a 40-minute stop, subtract the stopping time from the total time to get the actual driving time. Convert 40 minutes to hours (40/60 = 2/3 hours) so the effective travel time becomes 2 – 2/3 = 4/3 hours.\n",
       "3. Then, use the formula: speed = total distance / (total travel time – stopping time).\n",
       "\n",
       "Answer:\n",
       "• Without stopping: Average speed = 100 ÷ 2 = 50 mph.\n",
       "• With a 40-minute stop: Average speed = 100 ÷ (4/3) = 75 mph."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = pick_model(\"openai\",\"reason\")\n",
    "llm = LLMClient(\"openai\", model)\n",
    "\n",
    "problem = \"\"\" \n",
    "            A car travels 100 miles in 2 hours. What is the average speed of the car?\n",
    "            also if car stops for 40 minutes, what is the average speed of the car? \n",
    "        \"\"\"\n",
    "\n",
    "instruction = \"\"\" \n",
    "                Solve the following problem step by step.\n",
    "                    1. First identify whether car travelled the entire time without stopping or not. \n",
    "                    2. If car stopped for x minutes and overall for y the travel duration is y-x. So the speed should be d/(y-x).\n",
    "                    3. If stopping time x mentioned do not add it to the travel duration. Because it's already included in the travel duration.\n",
    "                        So actual travel time is y (total travel time) - x (stopping time).\n",
    "                    4. If car travelled the entire time without stopping, then the speed is d/y.\n",
    "                    5. If car stopped for x minutes, then the speed is d/(y-x).\n",
    "            \"\"\"\n",
    "\n",
    "prompt_text, spec = render(\n",
    "                            \"cot_reasoning.v1\",\n",
    "                            role = \"math_tutor\",\n",
    "                            problem = problem\n",
    "                            )\n",
    "\n",
    "messages = [\n",
    "            {\n",
    "                \"role\" : \"user\",\n",
    "                \"content\" : f\"\"\"\n",
    "                                text : {prompt_text}\\n\n",
    "                                instruction : {instruction}\n",
    "                            \"\"\"\n",
    "            }\n",
    "]\n",
    "\n",
    "response = llm.chat(messages, temperature=0.3)\n",
    "print(response['text'])\n",
    "# display(Markdown(response['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846fb3f",
   "metadata": {},
   "source": [
    "### 04 - TOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55f48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
